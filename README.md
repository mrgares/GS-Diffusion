
# GS-Diffusion

Diffusion model to refine output from a Gaussian Splatting-based model for autonomous driving use case.

This project trains a conditional diffusion model to enhance novel-view renderings (e.g., from DrivingForward or similar 3DGS models) by learning to map artifact-prone renderings to their corresponding high-quality ground-truth camera views.

---

## ğŸš€ Features

- Conditional image-to-image refinement using a shallow diffusion U-Net
- Designed for Gaussian Splatting pipelines in autonomous driving
- Docker-ready, GPU-accelerated environment with PyTorch and CUDA
- Based on Hugging Face's `diffusers` library

---

## ğŸ“¦ Requirements

All dependencies are handled via Docker. You just need:

- Docker
- NVIDIA Container Toolkit (for GPU access)
- A dataset structured as:

```

data/train/
â”œâ”€â”€ inputs/    # Rendered views from 3DGS (e.g., DrivingForward)
â”‚   â”œâ”€â”€ 0001.png
â”‚   â”œâ”€â”€ 0002.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ targets/   # Ground-truth camera views
â”‚   â”œâ”€â”€ 0001.png
â”‚   â”œâ”€â”€ 0002.png
â”‚   â””â”€â”€ ...

````

---

## âš™ï¸ Quick Start

### 1. Clone and build

```bash
git clone https://github.com/your-username/gs-diffusion.git
cd gs-diffusion
docker-compose build
````

### 2. Launch the container

```bash
docker-compose up -d
```

### 3. Attach to the container

```bash
docker exec -it diffusion_env bash
```

---



## ğŸ“ Project Structure

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train/
â”‚       â”œâ”€â”€ inputs/
â”‚       â””â”€â”€ targets/
â””â”€â”€ README.md
```

## ğŸ“¬ License

This project is licensed under the [MIT License](./LICENSE). See the LICENSE file for details.

